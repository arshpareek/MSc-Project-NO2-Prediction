{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa68fa-6910-4f0f-a2e8-a499401ddbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentinelsat\n",
    "!pip install ipywidgets\n",
    "!pip install --upgrade attrs\n",
    "!pip install h5netcdf\n",
    "!pip install netCDF4\n",
    "!pip install scipy\n",
    "!pip uninstall xarray -y\n",
    "!pip install xarray\n",
    "!pip install matplotlib\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-gpu\n",
    "!pip install -q -U keras-tuner\n",
    "!conda install -c conda-forge harp -y\n",
    "\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "from glob import iglob\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d0bd4-8d87-4471-9723-38afdb96dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "from glob import iglob\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "no_of_samples = 1529\n",
    "channels = 2\n",
    "image_dim = 16\n",
    "\n",
    "\n",
    "input_length = 10\n",
    "output_length = 5\n",
    "\n",
    "product_path_NO2 = \"Dataset_NO2_Processed/\"\n",
    "product_path_LST = \"Dataset_LST_Processed/\"\n",
    "\n",
    "input_files_NO2 = sorted(list(iglob(join(product_path_NO2, '*NO2*.npy'), recursive=False)))\n",
    "input_files_LST = sorted(list(iglob(join(product_path_LST, '*LST*.npy'), recursive=False)))\n",
    "\n",
    "no_of_samples = len(input_files_NO2)\n",
    "no_train_samples = int(no_of_samples*0.8)\n",
    "no_test_samples = no_of_samples - no_train_samples\n",
    "print(\"Number of samples: \", no_of_samples)\n",
    "\n",
    "train_samples = np.zeros(shape=(no_train_samples,image_dim,image_dim,channels))\n",
    "test_samples = np.zeros(shape=(no_test_samples,image_dim,image_dim,channels))\n",
    "count = 0\n",
    "\n",
    "#Stack the NO2 and LST data in separate channels of the same frame.\n",
    "for NO2 in input_files_NO2:\n",
    "    image_no2 = np.load(NO2)\n",
    "    #21-29 for new 15-23 for old\n",
    "    image_LST = np.load(join(product_path_LST, NO2[(len(product_path_NO2)+21):(len(product_path_NO2)+29)]) + \"_LST.npy\")\n",
    "    \n",
    "    if image_no2.shape == (1,image_dim,image_dim):\n",
    "        image_no2 = image_no2.reshape(image_dim,image_dim,1)\n",
    "        image_LST = image_LST.reshape(image_dim,image_dim,1)\n",
    "        \n",
    "        if count < no_train_samples:\n",
    "            train_samples[count] = (np.concatenate((image_no2, image_LST), axis=2))\n",
    "        else:\n",
    "            test_samples[count-no_train_samples] = (np.concatenate((image_no2, image_LST), axis=2))\n",
    "        \n",
    "        \n",
    "    count = count + 1\n",
    "    \n",
    "train_samples.clip(min=0, out=train_samples)\n",
    "test_samples.clip(min=0, out=test_samples)\n",
    "\n",
    "# Computing statistics for normalisation.\n",
    "min = np.nanmin(train_samples, axis = (0,1,2))\n",
    "max = np.nanmax(train_samples, axis = (0,1,2))\n",
    "mean = np.nanmean(train_samples, axis = (0,1,2))\n",
    "std = np.nanstd(train_samples, axis = (0,1,2))\n",
    "\n",
    "train_samples = train_samples - min\n",
    "train_samples = train_samples/(max-min)\n",
    "test_samples = test_samples - min\n",
    "test_samples = test_samples/(max-min)\n",
    "\n",
    "train_samples = np.nan_to_num(train_samples)\n",
    "test_samples = np.nan_to_num(test_samples)\n",
    "\n",
    "train_samples = np.clip(train_samples, 0, 1, out=train_samples)\n",
    "test_samples = np.clip(test_samples, 0, 1, out=test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24830a18-bd33-4f65-9fbd-89c3f8f168c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for Figure 4.3\n",
    "#plt.hist(train_samples[:,:,:,0].reshape(len(train_samples),-1))\n",
    "#plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9baf8-0eb5-4939-bd16-e4b7883d4e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4D tensor to 5D tensor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Create model inputs and corresponding labels using bundles of sequential data.\n",
    "# The following code in this cell has been adapted from the work of Muthukumar et. al.\n",
    "train_bundles = np.empty((no_train_samples-input_length,input_length,image_dim,image_dim,channels))\n",
    "for i in range(no_train_samples-input_length):\n",
    "    bundle = np.array([train_samples[i + k] for k in range(input_length)])\n",
    "    train_bundles[i] = bundle\n",
    "no_train_samples = train_bundles.shape[0]\n",
    "    \n",
    "test_bundles = np.empty((no_test_samples-input_length,input_length,image_dim,image_dim,channels))\n",
    "for i in range(no_test_samples-input_length):\n",
    "    bundle = np.array([test_samples[i + k] for k in range(input_length)])\n",
    "    test_bundles[i] = bundle\n",
    "no_test_samples = test_bundles.shape[0]\n",
    "\n",
    "X_train = train_bundles[:no_train_samples-(input_length)]\n",
    "X_val = test_bundles[:int(no_test_samples*0.5)]\n",
    "X_test = test_bundles[int(no_test_samples*0.5)+input_length:no_test_samples-(input_length)]\n",
    "\n",
    "y_train = np.expand_dims(train_bundles[input_length:,:output_length,:,:,0],-1)\n",
    "y_val = np.expand_dims(test_bundles[input_length:int(no_test_samples*0.5)+input_length,:output_length,:,:,0],-1)\n",
    "y_test = np.expand_dims(test_bundles[int(no_test_samples*0.5)+2*input_length:,:output_length,:,:,0],-1)\n",
    "\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=7)\n",
    "\n",
    "print(X_train.shape, \"X_train shape\")\n",
    "print(X_val.shape, \"X_val shape\")\n",
    "print(X_test.shape, \"X_test shape\")\n",
    "\n",
    "print(y_train.shape, \"y_train shape\")\n",
    "print(y_val.shape, \"y_val shape\")\n",
    "print(y_test.shape, \"y_test shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af326ee-1285-4985-b403-e18d6577dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-gpu\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import ConvLSTM2D, Input\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.backend import expand_dims, repeat_elements\n",
    "from keras.layers import Lambda\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# Implementation of the shrinkage loss function.\n",
    "def shrinkage_loss(y_true, y_pred):\n",
    " \n",
    "    error = tf.subtract(y_pred, y_true)\n",
    "    squared_error = K.square(error) \n",
    "    \n",
    "    a = 10.0\n",
    "    c = 0.1\n",
    "    \n",
    "    shrinkage_loss = (squared_error/(1 + K.exp(a * (c - error))))\n",
    "                \n",
    "    loss = tf.reduce_mean(shrinkage_loss, axis=(1,2,3,4))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Implementation of the shrinkage version of the mean absolute loss.\n",
    "def shrinkage_absolute_loss(y_true, y_pred):\n",
    " \n",
    "    error = y_pred - y_true\n",
    "    squared_error = K.abs(error) \n",
    "    \n",
    "    a = 10.0\n",
    "    c = 0.3\n",
    "    \n",
    "    shrinkage_loss = squared_error/(1 + K.exp(a * (c - error)))\n",
    "    \n",
    "    loss = K.mean(shrinkage_loss, axis=(1,2,3,4))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def repeatV(x):\n",
    "    return repeat_elements(expand_dims(x, axis=1), output_length, 1)\n",
    "\n",
    "# Implementation of the Convolutional LSTM model.\n",
    "inputs = Input(shape=(input_length, image_dim, image_dim, channels))\n",
    "enc_layer_1 = ConvLSTM2D(filters=32, kernel_size=7,\n",
    "                   input_shape=(input_length, image_dim, image_dim, channels),\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(inputs)\n",
    "enc_batch_norm_1 = BatchNormalization()(enc_layer_1)\n",
    "\n",
    "enc_layer_2 = ConvLSTM2D(filters=64, kernel_size=7,\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(enc_batch_norm_1)\n",
    "enc_batch_norm_2 = BatchNormalization()(enc_layer_2)\n",
    "\n",
    "enc_layer_3 = ConvLSTM2D(filters=128, kernel_size=9,\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(enc_batch_norm_2)\n",
    "enc_batch_norm_3 = BatchNormalization()(enc_layer_3)\n",
    "\n",
    "enc_layer_4 = ConvLSTM2D(filters=128, kernel_size=9,\n",
    "                   padding='same', return_sequences=False, data_format='channels_last')(enc_batch_norm_3)\n",
    "enc_batch_norm_4 = BatchNormalization()(enc_layer_4)\n",
    "\n",
    "repeat = Lambda(repeatV)(enc_batch_norm_4)\n",
    "\n",
    "dec_layer_1 = ConvLSTM2D(filters=128, kernel_size=11,\n",
    "                   input_shape=(output_length, image_dim, image_dim, channels),\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(repeat)\n",
    "dec_batch_norm_1 = BatchNormalization()(dec_layer_1)\n",
    "\n",
    "dec_layer_2 = ConvLSTM2D(filters=128, kernel_size=9,\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(dec_batch_norm_1)\n",
    "dec_batch_norm_2 = BatchNormalization()(dec_layer_2)\n",
    "\n",
    "dec_layer_3 = ConvLSTM2D(filters=64, kernel_size=9,\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(dec_batch_norm_2)\n",
    "dec_batch_norm_3 = BatchNormalization()(dec_layer_3)\n",
    "\n",
    "dec_layer_4 = ConvLSTM2D(filters=32, kernel_size=7,\n",
    "                   padding='same', return_sequences=True, data_format='channels_last')(dec_batch_norm_3)\n",
    "dec_batch_norm_4 = BatchNormalization()(dec_layer_4)\n",
    "\n",
    "y = Conv2D(filters=1, kernel_size=9,\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last')(dec_batch_norm_4)\n",
    "\n",
    "seq = Model(inputs, y)\n",
    "\n",
    "# Different loss functions, e.g. shrinkage_loss, \"mean_squared_error\", etc., can be used to compile the model.\n",
    "seq.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.90, beta_2=0.999, amsgrad=False), metrics=[\"mean_absolute_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7174857-20f9-4982-aa07-9408dac0d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was used to set up the hyperparameter tuner.\n",
    "\n",
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "# !pip install tensorflow-gpu\n",
    "# !pip install -q -U keras-tuner\n",
    "# import keras_tuner as kt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# import keras\n",
    "# from keras.models import Model\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "# from keras.layers import ConvLSTM2D, Input\n",
    "# from keras.layers import RepeatVector\n",
    "# from keras.layers import BatchNormalization\n",
    "\n",
    "# from keras.backend import expand_dims, repeat_elements\n",
    "# from keras.layers import Lambda\n",
    "\n",
    "# import keras.backend as K\n",
    "\n",
    "# def shrinkage_loss(y_true, y_pred):\n",
    " \n",
    "#     error = y_pred - y_true\n",
    "#     squared_error = K.square(y_pred - y_true)\n",
    "    \n",
    "#     a = 10.0\n",
    "#     c = 0.1\n",
    "    \n",
    "#     shrinkage_loss = squared_error/(1 + K.exp(a * (c - error)))\n",
    "                \n",
    "#     loss = K.mean(shrinkage_loss, axis=(1,2,3,4))\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# def repeatV(x):\n",
    "#     return repeat_elements(expand_dims(x, axis=1), output_length, 1)\n",
    "\n",
    "# def model_builder_enc_dec(hp):\n",
    "    \n",
    "#     inputs = Input(shape=(input_length, image_dim, image_dim, channels))\n",
    "#     layer_input = inputs\n",
    "#     num_enc_layers = hp.Int('enc_layers', 1, 8)\n",
    "#     num_dec_layers = hp.Int('dec_layers', 1, 8)\n",
    "#     for i in range(num_enc_layers):\n",
    "#         if(i == num_enc_layers-1):\n",
    "#             return_sequence = False\n",
    "#         else:\n",
    "#             return_sequence = True\n",
    "    \n",
    "#         layer = ConvLSTM2D(filters=hp.Choice(f'filters_enc_{i}', [8,16,32,64,128]), kernel_size=hp.Int(f'kernel_enc_{i}', min_value=3, max_value=11, step=2),\n",
    "#                            input_shape=(input_length, image_dim, image_dim, channels),\n",
    "#                            padding='same', return_sequences=return_sequence, data_format='channels_last')(layer_input)\n",
    "#         batch_norm = BatchNormalization()(layer)\n",
    "        \n",
    "#         layer_input = batch_norm\n",
    "\n",
    "#     repeat = Lambda(repeatV)(layer_input)\n",
    "    \n",
    "#     layer_input = repeat\n",
    "    \n",
    "#     for i in range(num_enc_layers):\n",
    "#         layer = ConvLSTM2D(filters=hp.Choice(f'filters_dec_{i}', [8,16,32,64,128]), kernel_size=hp.Int(f'kernel_dec_{i}', min_value=3, max_value=11, step=2),\n",
    "#                            input_shape=(input_length, image_dim, image_dim, channels),\n",
    "#                            padding='same', return_sequences=True, data_format='channels_last')(layer_input)\n",
    "#         batch_norm = BatchNormalization()(layer)\n",
    "        \n",
    "#         layer_input = batch_norm\n",
    "\n",
    "#     kernel = hp.Int('kernel', min_value=3, max_value=11, step=2)\n",
    "        \n",
    "#     y = Conv2D(filters=1, kernel_size=kernel,\n",
    "#                    activation='sigmoid',\n",
    "#                    padding='same', data_format='channels_last')(layer_input)\n",
    "    \n",
    "#     seq = Model(inputs, y)\n",
    "    \n",
    "#     learning_rate = hp.Choice('learning_rate', [0.005, 0.001, 0.0005, 0.0001, 0.00005])\n",
    "#     beta_1 = hp.Choice('beta_1', [0.7, 0.8, 0.9, 0.999])\n",
    "#     beta_2 = hp.Choice('beta_2', [0.7, 0.8, 0.9, 0.999])\n",
    "    \n",
    "#     seq.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, amsgrad=False), metrics=[\"mean_absolute_error\"])\n",
    "    \n",
    "#     return seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd64ce3-7a0a-41a7-a324-2343e5d1c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was used to run the hyperparameter tuner.\n",
    "\n",
    "# tuner = kt.Hyperband(model_builder_enc_dec,\n",
    "#                      objective=kt.Objective(\"val_mean_absolute_error\", direction=\"min\"),\n",
    "#                      max_epochs=40,\n",
    "#                      factor=3,\n",
    "#                      directory='tuner',\n",
    "#                      project_name='ConvLSTM_Tune')\n",
    "\n",
    "# tuner.search(X_train, y_train, epochs=30, validation_data=(X_val, y_val))\n",
    "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc1852-c7f8-475c-b11b-35f0f95be0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model.\n",
    "# Choose model from:\n",
    "# 'saved_models/model_mse'\n",
    "# 'saved_models/model_shrinkage'\n",
    "# 'saved_models/model_wide'\n",
    "\n",
    "model_path = 'saved_models/model_shrinkage'\n",
    "\n",
    "json_file = open(model_path + '.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "seq = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "seq.load_weights(model_path + \".h5\")\n",
    "print(\"Model Loaded\")\n",
    "seq.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Use following code to train model from scratch.\n",
    "# seq.summary()\n",
    "# history = seq.fit(X_train, y_train, batch_size=32, epochs=30, validation_data=(X_val, y_val))\n",
    "# Y_hat = seq.predict(X_test, verbose=0)\n",
    "# print(Y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ae238-9369-4387-afbc-5616dc9b8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for saving model and weights.\n",
    "# model_json = seq.to_json()\n",
    "# with open(\"model_LST.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "    \n",
    "# # Save weights to HDF5\n",
    "# seq.save_weights(\"model_LST.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1deed-fe4c-40c2-853b-bf45b3ca4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following visualization code in this cell is taken from Muthukumar et. al's work.\n",
    "# Randomly visualizes frame sequences from the test set.\n",
    "def visualization(y, y_hat, n_slots):\n",
    "    \n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    length = y_hat.shape[0]\n",
    "    random_ind = [x for x in range(0,n_slots)]\n",
    "    setCount = 1;\n",
    "    for ind in random_ind:\n",
    "        setCount = setCount+1;\n",
    "        fig, (ax1, ax2)= plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "\n",
    "        y_pre = y_hat[ind,0,:,:,0]\n",
    "        ax1.imshow(y_pre)\n",
    "\n",
    "        y_truth = y[ind,0,:,:,0]\n",
    "        ax2.imshow(y_truth)\n",
    "\n",
    "        plt.text(2, -0.8, 'Ground truth Frame 1', fontsize=13)\n",
    "    \n",
    "        fig, (ax1, ax2)= plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "        \n",
    "        y_pre = y_hat[ind,1,:,:,0]\n",
    "        ax1.imshow(y_pre)\n",
    "        \n",
    "        y_truth = y[ind,1,:,:,0]\n",
    "        ax2.imshow(y_truth)\n",
    "\n",
    "        plt.text(2, -0.8, 'Ground truth Frame 2', fontsize=13)\n",
    "    \n",
    "    \n",
    "        fig, (ax1, ax2)= plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "        \n",
    "        y_pre = y_hat[ind,2,:,:,0]\n",
    "        ax1.imshow(y_pre)\n",
    "\n",
    "        y_truth = y[ind,2,:,:,0]\n",
    "        ax2.imshow(y_truth)\n",
    "\n",
    "        plt.text(2, -0.8, 'Ground truth Frame 3', fontsize=13)\n",
    "    \n",
    "        fig, (ax1, ax2)= plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "\n",
    "        y_pre = y_hat[ind,3,:,:,0]\n",
    "        ax1.imshow(y_pre)\n",
    "\n",
    "        y_truth = y[ind,3,:,:,0]\n",
    "        ax2.imshow(y_truth)\n",
    "\n",
    "        plt.text(2, -0.8, 'Ground truth Frame 4', fontsize=13)\n",
    "    \n",
    "        fig, (ax1, ax2)= plt.subplots(1, 2, sharey=True, sharex=True, figsize=(10, 5))\n",
    "\n",
    "        y_pre = y_hat[ind,4,:,:,0]\n",
    "        ax1.imshow(y_pre)\n",
    "\n",
    "        y_truth = y[ind,4,:,:,0]\n",
    "        ax2.imshow(y_truth)\n",
    "\n",
    "        plt.text(2, -0.8, 'Ground truth Frame 5', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9560905-cca8-4685-88d9-91db3a7a3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization(y_test, Y_hat, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23537c62-ff62-42e3-805d-cc1c935fffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Calculate raw normalised statistics for evaluation.\n",
    "\n",
    "Y_hat = seq.predict(X_test, verbose=0)\n",
    "\n",
    "img_dim = 16\n",
    "resi = []\n",
    "hati = []\n",
    "tesi = []\n",
    "\n",
    "count = 0\n",
    "for j in range(0, 5, 2):\n",
    "    lst = []\n",
    "    lstres = []\n",
    "    lstresidual = []\n",
    "    hat = []\n",
    "    test = []\n",
    "    for ind in range(Y_hat.shape[0]):\n",
    "\n",
    "        img_hat = Y_hat[ind][j].reshape(img_dim,img_dim).astype('float64')\n",
    "        img_test = y_test[ind][j].reshape(img_dim,img_dim)\n",
    "\n",
    "        mae = mean_absolute_error(img_hat, img_test)\n",
    "        res = np.mean(img_hat-img_test)\n",
    "        residual = img_hat-img_test\n",
    "        \n",
    "        lstresidual.append(residual)\n",
    "        lst.append(mae)\n",
    "        lstres.append(res)\n",
    "        hat.append(img_hat)\n",
    "        test.append(img_test)\n",
    "    \n",
    "    resi.append(lstresidual)\n",
    "    hati.append(hat)\n",
    "    tesi.append(test)\n",
    "    \n",
    "    lst = np.array(lst)\n",
    "    mean = np.mean(lst)\n",
    "    std = np.std(lst)\n",
    "    meansim = np.mean(lstsim)\n",
    "    stdsim = np.std(lstsim)\n",
    "    meanres = np.mean(lstres)\n",
    "    stdres = np.std(lstres)\n",
    "    \n",
    "    print(\"mean\", mean)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e3a96-2271-438d-83d6-30bc89472fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temporal average residual errors.\n",
    "\n",
    "avg = np.mean(resi, axis=1)\n",
    "\n",
    "f, axarr = plt.subplots(1,len(avg))\n",
    "for frame_ind in range(len(avg)):\n",
    "    \n",
    "    data_mean = 119\n",
    "    b = axarr[frame_ind].imshow(avg[frame_ind]*data_mean, interpolation='none')\n",
    "    b.axes.get_xaxis().set_visible(False)\n",
    "    b.axes.get_yaxis().set_visible(False)\n",
    "    axarr[frame_ind].title.set_text('Day ' + str(2*(frame_ind + 1)-1))\n",
    "   \n",
    "\n",
    "cbar = f.colorbar(b, ax=axarr.ravel().tolist(), orientation=\"horizontal\")\n",
    "cbar.set_label('Mean Error (Petamolecules/cm2)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
